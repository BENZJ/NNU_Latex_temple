%% 英文摘要
%% Abstract Times New Roman 字体,四号加粗。1.25 倍行距
\chapter*{\centerline{\textbf{ \zihao{4} Abstract}}}
\chaptermark{Abstract}
\thispagestyle{nnu@plain}
\addcontentsline{toc}{chapter}{Abstract}
\linespread{1.25} 
\vspace{1em}
{\zihao{-4} 
In recent years, with the rapid development of deep learning in various fields of society, its security has attracted more and more attention. Adversarial examples have brought a lot of potential threats in deep learning applications and real life. The adversarial sample that is to add a deliberate perturbation to the original image to make a specific deep learning classifier produce a wrong classification output with high confidence. The adversarial examples can not only deceive the machine learning classifier, but also the difference should not be conscious of by human vision directly. Through the research of adversarial example attack algorithms, the defense algorithms and the security of deep learning applications can be continuously improved. In current algorithms, norms are usually used to calculate the perturbation of adversarial samples, such as C\&W and Deepfool algorithms. However, the obtained adversarial samples from these algorithms often have abnormal textures that can be observed by human beings in the smooth areas of some images, resulting in mismatch of visual distance. In order to solve these problems, our research proposed an image adversarial example generation algorithm based on texture, perceptual color distance and hyperparameter adaptive adjustment, which can effectively embed perturbation to the high-texture area of the original image. 
    


First, a new loss function is constructed through the perceptual color distance to replace the norm loss, and the color distance is continuously optimized in the iterative process, avoiding the difference of visual distance between the original image and the adversarial example, and also avoid the risk of the adversarial examples being visually recognized. At the same time, based on the Laplacian filter and the bilateral filter, two algorithms of texture amplification are proposed to effectively highlight the abnormal textures in the adversarial examples. Compared with the C\&W and DDN algorithms, the adversarial examples generated by the algorithm (T-pcd) from our research have the better imperceptibility and lower chromatic aberration distance. Only modifying 70\% of the pixels in an original image, the attack can be successful, and the T-pcd method can effectively fight against JPEG and Bit Depth compression. In addition, there is a certain degree of mobility between different deep network models.

First, a new loss function is constructed through the perceptual color distance to replace the norm loss, and the color distance is continuously optimized in the iterative process, avoiding the difference of visual distance between the original image and the adversarial example, and also avoid the risk of the adversarial examples being visually recognized. At the same time, based on the Laplacian filter and the bilateral filter, two algorithms of texture amplification are proposed to effectively highlight the abnormal textures in the adversarial examples. Compared with the C\&W and DDN algorithms, the adversarial examples generated by the algorithm (T-pcd) from our research have the better imperceptibility and lower chromatic aberration distance. Only modifying 70\% of the pixels in an original image, the attack can be successful, and the T-pcd method can effectively fight against JPEG and Bit Depth compression. In addition, there is a certain degree of mobility between different deep network models.







\vspace{1em}
\noindent\textbf{Keywords}:~~Machine Learning,~~Regularization,~~Video Compression
\thispagestyle{nnu@plain}

\linespread{1.5} 
